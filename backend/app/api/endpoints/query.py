# app/api/endpoints/query.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from backend.app.core.llm_chain import create_qa_chain
from backend.app.core.vectorstore import create_vectorstore
from langchain_core.documents import Document
from urllib.parse import unquote
from langchain_openai import ChatOpenAI
from elasticsearch import Elasticsearch
import os

router = APIRouter()

class QueryRequest(BaseModel):
    question: str

@router.post("/ask")
async def ask_question(request: QueryRequest):
    qa_chain = create_qa_chain()
    answer = qa_chain.run(request.question)
    return {"answer": answer}

@router.post("/ask/{filename}")
async def ask_file(filename: str, request: QueryRequest):
    try:
        filename = unquote(filename)  # URL ë””ì½”ë”© ì¶”ê°€
        vectorstore = create_vectorstore()
        # ----------------------------------------------------------------------------------
        # filename ë©”íƒ€ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰
        # docs = vectorstore.similarity_search(
        #     query=request.question,
        #     filter={"filename": filename},  # ë©”íƒ€ë°ì´í„°ì— filename í•„í„° ì ìš©
        #     k=3  # top 3 ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        # )
        # ----------------------------------------------------------------------------------
        # ì§ì ‘ filter ì—†ì´ ê²€ìƒ‰ í›„, ìˆ˜ë™ í•„í„°ë§ ì‹œë„
        # docs = vectorstore.similarity_search(
        #     query=request.question,
        #     k=10  # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼ ì¶©ë¶„íˆ í™•ë³´
        # )

        # ìˆ˜ë™ í•„í„°ë§
        # filtered_docs = [doc for doc in docs if doc.metadata.get("filename") == filename]

        # if not docs:
        #     return {"answer": "ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # ë¬¸ì„œë“¤ì„ ì—°ê²°í•´ì„œ ë‹µë³€ ìƒì„±
        # combined_content = "\n".join(doc.page_content for doc in docs)
        # return {"answer": combined_content}
        # ----------------------------------------------------------------------------------
        question = request.question
        # ğŸ”¥ Elasticsearch ì§ì ‘ raw query
        es_query = {
            "size": 10,
            "query": {
                "bool": {
                    "must": [
                        {"match": {"content": question}}
                    ],
                    "filter": [
                        {"term": {"metadata.filename.keyword": filename}}
                    ]
                }
            }
        }
        response = vectorstore.search(index="rag-index", body=es_query)

        hits = response["hits"]["hits"]
        if not hits:
            return {"answer": "ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "suggestions": []}

        combined_text = "\n".join(hit["_source"]["content"] for hit in hits)

        # ë‹µë³€ ìš”ì•½ (LLM ì‚¬ìš©)
        llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
        summary_prompt = f"ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ ìš”ì•½ ë‹µë³€ì„ ì‘ì„±í•´ì¤˜.\n\në¬¸ì„œ ë‚´ìš©:\n{combined_text}\n\nì§ˆë¬¸: {question}\n\nìš”ì•½ ë‹µë³€:"
        summary_response = llm.invoke(summary_prompt)
        summary = summary_response.content.strip()

        # ì¶”ê°€ ì§ˆë¬¸ ì¶”ì²œ
        followup_prompt = f"ì‚¬ìš©ìê°€ '{question}' ë¼ê³  ì§ˆë¬¸í–ˆì–´. ì´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ë¡œ í•  ë§Œí•œ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•´ì¤˜. ì§§ê³  ëª…í™•í•˜ê²Œ."
        followup_response = llm.invoke(followup_prompt)
        followup_questions = followup_response.content.strip().split('\n')

        return {
            "answer": summary,
            "suggestions": [q.strip('- ').strip() for q in followup_questions if q.strip()]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")